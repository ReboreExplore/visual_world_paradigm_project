---
title: "**Visual World Paradigm**"
subtitle: "*A classical visual world study showing how people predict upcoming words with the help of Gazepoint eye tracker*"
author:
  - name: Pritom Gogoi, Manpa Barman, Kapil Chander Mulchandani
    affiliation: University of Stuttgart
    affiliation-url: https://www.uni-stuttgart.de/en/
date: today
abstract: The study presented in this paper explores the dynamics of predictive language processing through the visual world paradigm (VWP), a widely employed method in cognitive psychology. The primary objective of the research is to unwind how individuals anticipate or predict forthcoming words during the unfolding of the spoken instructions, leveraging the Gazepoint eye tracker for precise gaze pattern analysis. The investigation delves into the impact of competitor words on gaze patterns, to study the cognitive mechanisms underlying real-time language comprehension. Our experiment uses a collection of competitor words sharing phonetic or semantic similarities with the target, and validates the hypothesis that the existence of such competitors leads to an increased number of fixations on them, reflecting the participants' evolving predictions of the upcoming word.
toc: true 
bibliography: biblio.bib
---

[Download the pdf version.](report/docs/report.pdf)

# Introduction

## Visual World Paradigm

The visual world paradigm is an experimental framework that investigates language processing by monitoring participants' eye movements while they interact with visual stimuli. Introduced by psychologists Richard Cooper and Thomas P. McDermott in the late 1990s, this paradigm have been continuosly refined and expanded, adapting it to different research questions and using advancements in eye-tracking technology to gain deeper insights into real-time language comprehension and visual attention processes. Through this framework the researchers try to simulate the integration of spoken language and visual information as they naturally occur in everyday situations so that we can draw inferences on the attention focus on specific objects in their visual display over time. 

## Objective of our project
We try to answer the research question: 

*Does the presence of similar-sounding words influence our tendency to focus on those words apart from the target words as the word unfolds?*


Our project is to study the nature of spoken word recognition as the word unfolds. Here the key aspect of the visual world paradigm is that participants' eye movements serve as an index of their ongoing language processing and interpretation. 

We aim to explore two fundamental conclusions concerning spoken word recognition and the underlying models, building upon the established research in this domain: 

- Spoken word recognition is dynamic in nature which suggests that listeners continuously update and refine their interpretations as more information becomes available.
- Spoken word recognition models make assumptions that multiple candidates compete for recognition during the unfolding of the spoken word.

Paul D.Allopenna, James S. Magnuson and  Michael K. Tanenhaus in their paper *"Tracking the Time Course of Spoken Word Recognition Using Eye Movements: Evidence for Continuous Mapping Models"* investigated a similar structure of the experiment and found the following results: 

![Probability of fixating on each item type over time in the full competitor condition](img/ref_graph.png){#fig-matrix width=90%}

In this figure we have the probability of fixation on four words:
- Referent (e.g beaker) : Target Word
- Cohort (e.g beetle) : Similar Sounding Word
- Rhyme (e.g speaker) : Rhyming word
- Unrelated (e.g carriage) : Unrelated word to the rest (phonetically or semantically.)

In the beginning the participants hear [bi], which could be the beginning of *beaker* but also could be the beginning of *beetle*. So during the first 400 ms the particpants start looking at both of those words, more than they look at the others. After some time as they hear the [k] i.e. now they are hearing [bik], thus they discard their choice of *beetle* and stop looking at it. But by the time they've heard the whole word *beaker*, they might realize that *beaker* rhymes almost exactly with *speaker* and get confused about if they heard *speaker* at the very first place. For the last word carriage the pronunciation is totally unrelated to the target *beaker*, so there is a very less probability of the participant actually fixation at the unrelated word.

<<<<<<< HEAD
{{< include _pritom.qmd >}}

------------------------------------
=======

## Experiment 

### Software and Hardware

- **Software** : The experiment is designed in [OpenSesame](https://osdoc.cogsci.nl/) which is a open source software to create experiments for psychology, neuroscience, and experimental economics. 

- **Language** : Python was used along with the OpenSesame GUI to create the experiment. Libraries like Pandas, Numpy, Matplotlib were used to analyze the data. For backend processing in the OpenSesame GUI, PsychoPy was used.

- **Eye Tracker**: The experiment is conducted using the GazePoint GP3+ eye tracker. It is a binocular eye tracker that can record at 150 Hz  The eye tracker is connected to the computer and the participants are seated at a distance of 60 cm from the screen. The experiment was conducted in a dimly lit laboratory setup to avoid any external light source that might interfere with the eye tracking.

### Structure of the Stimulus

The experiment is designed to test the participants' ability to predict the upcoming word in a spoken instruction. The experiment is designed in such a way that the participants are presented with a visual display of four objects in a grid and they are instructed to click on the object that matches the spoken instruction.

A trial in the experiment means the response to one spoken instruction.

The following User Interface is presented to the participants:

![User Interface of one trial - In each trial we have a 3x3 grid structure with four objects in it. Each trial has four stimulus displayed. The four stimulus are a combination of referent, cohort, rhyme and unrelated words. All or atleast two of the combinations are present in each trial according to the condition set (Full competitor, Rhyme competitor, Unrelated competitor etc.), which will be referred in the later part of the report. The stimulus used are only line drawings of the objects, to remove anmibuity in the visual display. The dot in the center grid is the fixation point for the participants.](img/stim_grid.png){#fig-ui width=90%}

### Design of the Experiment

The experiment follows the following structure in OpenSesame:
1. **Introduction to the experiment** : It contains some preliminary instructions for the participants to understand the experiment. It also mentions that each progression will require a mouse click. The foreground color of the text is set to black and the background color is set to white throughout the experiment.

![Introduction](img/intro_script.png.png){#fig-ui width=90%}

2. **Initialization of variables** : The position variables (Top, Buttom, Left and Right) are initialized and are used to set the position of the objects in the grid. The pygaze module is also intialized to record the eye movements of the participants.




References: [@Vitay2017]

See @fig-matrix and @sec-results.



$$
    \tau \, \frac{dx_j(t)}{dt} + x_j(t)= \sum_i w^{in}_{ij} \, r^{in}_i(t) + g \, \sum_{i \neq j} w^{rec}_{ij} \, r_i(t)
$$

::: {.callout-note}
## Nota Bene

Important information.
:::

```python
for i in range(10):
    print(i)
```

## Second subsection

{{< video https://www.youtube.com/embed/tPgf_btTFlc >}}

# Material and methods

# Results {#sec-results}

# Discussion

# References {.unnumbered}
