% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  a4paper,
]{article}

\usepackage{amsmath,amssymb}
\usepackage{setspace}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{times}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{\underline{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{\textbf{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.44,0.26,0.76}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.14,0.16,0.18}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.14,0.16,0.18}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.44,0.26,0.76}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.89,0.38,0.04}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{#1}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Visual World Paradigm},
  pdfauthor={Pritom Gogoi, Manpa Barman, Kapil Chander Mulchandani},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{\textbf{Visual World Paradigm}}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{\emph{A classical visual world study showing how people
predict upcoming words with the help of Gazepoint eye tracker}}
\author{Pritom Gogoi, Manpa Barman, Kapil Chander Mulchandani}
\date{2023-08-22}

\begin{document}
\maketitle
\begin{abstract}
The study presented in this paper explores the dynamics of predictive
language processing through the visual world paradigm (VWP), a widely
employed method in cognitive psychology. The primary objective of the
research is to unwind how individuals anticipate or predict forthcoming
words during the unfolding of the spoken instructions, leveraging the
Gazepoint eye tracker for precise gaze pattern analysis. The
investigation delves into the impact of competitor words on gaze
patterns, to study the cognitive mechanisms underlying real-time
language comprehension. Our experiment uses a collection of competitor
words sharing phonetic or semantic similarities with the target, and
validates the hypothesis that the existence of such competitors leads to
an increased number of fixations on them, reflecting the participants'
evolving predictions of the upcoming word.
\end{abstract}
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[frame hidden, interior hidden, borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt, enhanced, sharp corners, breakable]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
\setstretch{1.25}
\href{report/docs/report.pdf}{Download the pdf version.}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\hypertarget{visual-world-paradigm}{%
\subsection{Visual World Paradigm}\label{visual-world-paradigm}}

The visual world paradigm is an experimental framework that investigates
language processing by monitoring participants' eye movements while they
interact with visual stimuli. Introduced by psychologists Richard Cooper
and Thomas P. McDermott in the late 1990s, this paradigm have been
continuosly refined and expanded, adapting it to different research
questions and using advancements in eye-tracking technology to gain
deeper insights into real-time language comprehension and visual
attention processes. Through this framework the researchers try to
simulate the integration of spoken language and visual information as
they naturally occur in everyday situations so that we can draw
inferences on the attention focus on specific objects in their visual
display over time.

\hypertarget{objective-of-our-project}{%
\subsection{Objective of our project}\label{objective-of-our-project}}

We try to answer the research question:

\emph{Does the presence of similar-sounding words influence our tendency
to focus on those words apart from the target words as the word
unfolds?}

Our project is to study the nature of spoken word recognition as the
word unfolds. Here the key aspect of the visual world paradigm is that
participants' eye movements serve as an index of their ongoing language
processing and interpretation.

We aim to explore two fundamental conclusions concerning spoken word
recognition and the underlying models, building upon the established
research in this domain:

\begin{itemize}
\tightlist
\item
  Spoken word recognition is dynamic in nature which suggests that
  listeners continuously update and refine their interpretations as more
  information becomes available.
\item
  Spoken word recognition models make assumptions that multiple
  candidates compete for recognition during the unfolding of the spoken
  word.
\end{itemize}

Paul D.Allopenna, James S. Magnuson and Michael K. Tanenhaus in their
paper \emph{``Tracking the Time Course of Spoken Word Recognition Using
Eye Movements: Evidence for Continuous Mapping Models''} investigated a
similar structure of the experiment and found the following results:

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{img/ref_graph.png}

}

\caption{\label{fig-matrix}Probability of fixating on each item type
over time in the full competitor condition}

\end{figure}

In this figure we have the probability of fixation on four words:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Referent (e.g beaker) : Target Word
\item
  Cohort (e.g beetle) : Similar Sounding Word
\item
  Rhyme (e.g speaker) : Rhyming word
\item
  Unrelated (e.g carriage) : Unrelated word to the rest (phonetically or
  semantically.)
\end{enumerate}

In the beginning the participants hear {[}bi{]}, which could be the
beginning of \emph{beaker} but also could be the beginning of
\emph{beetle}. So during the first 400 ms the particpants start looking
at both of those words, more than they look at the others. After some
time as they hear the {[}k{]} i.e.~now they are hearing {[}bik{]}, thus
they discard their choice of \emph{beetle} and stop looking at it. But
by the time they've heard the whole word \emph{beaker}, they might
realize that \emph{beaker} rhymes almost exactly with \emph{speaker} and
get confused about if they heard \emph{speaker} at the very first place.
For the last word carriage the pronunciation is totally unrelated to the
target \emph{beaker}, so there is a very less probability of the
participant actually fixation at the unrelated word.

\hypertarget{experiment}{%
\section{Experiment}\label{experiment}}

\hypertarget{software-and-hardware}{%
\subsection{Software and Hardware}\label{software-and-hardware}}

\begin{itemize}
\item
  \textbf{Software} : The experiment is designed in
  \href{https://osdoc.cogsci.nl/}{OpenSesame} which is a open source
  software to create experiments for psychology, neuroscience, and
  experimental economics.
\item
  \textbf{Language} : Python was used along with the OpenSesame GUI to
  create the experiment. Libraries like Pandas, Numpy, Matplotlib were
  used to analyze the data. For backend processing in the OpenSesame
  GUI, PsychoPy was used.
\item
  \textbf{Eye Tracker}: The experiment is conducted using the GazePoint
  GP3+ eye tracker. It is a binocular eye tracker that can record at 150
  Hz The eye tracker is connected to the computer and the participants
  are seated at a distance of 60 cm from the screen. The experiment was
  conducted in a dimly lit laboratory setup to avoid any external light
  source that might interfere with the eye tracking.
\end{itemize}

\hypertarget{structure-of-the-stimulus}{%
\subsection{Structure of the Stimulus}\label{structure-of-the-stimulus}}

The experiment is designed to test the participants' ability to predict
the upcoming word in a spoken instruction. The experiment is designed in
such a way that the participants are presented with a visual display of
four objects in a grid and they are instructed to click on the object
that matches the spoken instruction.

A trial in the experiment means the response to one spoken instruction.

The following User Interface is presented to the participants:

\begin{figure}

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{img/stim_grid.png}

}

\caption{\label{fig-ui}User Interface of one trial - In each trial we
have a 3x3 grid structure with four objects in it. Each trial has four
stimulus displayed. The four stimulus are a combination of referent,
cohort, rhyme and unrelated words. All or atleast two of the
combinations are present in each trial according to the condition set
(Full competitor, Rhyme competitor, Unrelated competitor etc.), which
will be referred in the later part of the report. The stimulus used are
only line drawings of the objects, to remove anmibuity in the visual
display. The dot in the center grid is the fixation point for the
participants.}

\end{figure}

\hypertarget{design-of-the-experiment}{%
\subsection{Design of the Experiment}\label{design-of-the-experiment}}

The experiment follows the following structure in OpenSesame:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Introduction to the experiment} : It contains some preliminary
  instructions for the participants to understand the experiment. It
  also mentions that each progression will require a mouse click. The
  foreground color of the text is set to black and the background color
  is set to white throughout the experiment.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{img/intro_script.png}

}

\caption{\label{fig-ui}Introduction}

\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \textbf{Initialization of variables} : The position variables (Top,
  Buttom, Left and Right) are initialized and are used to set the
  position of the objects in the grid. The pygaze module is also
  intialized to record the eye movements of the participants.
\item
  \textbf{Trial Loop Items} : This loop runs the experiment for 24
  trials. The trial loop contains the following sequence of events:

  \begin{itemize}
  \item
    \textbf{Fixation Cross} : A fixation cross is displayed at the
    center of the screen. The fixation cross is a black dot on a white
    background. The fixation cross is displayed to ensure that the
    participants are looking at the center of the screen before the
    spoken instruction is played. The fixation cross is displayed using
    the sketchpad item in OpenSesame.
  \item
    \textbf{Stimulus} :

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      The visual stimulus for each trial is loaded from the
      \texttt{stimuli.csv} file. The csv file contains information about
      the four objects that are displayed in the grid. The csv file
      contains the following information:

      \begin{itemize}
      \tightlist
      \item
        \textbf{Stimulus} : The name of the objects that is displayed in
        the grid.
      \item
        \textbf{Type} : The type of the object. The type can be
        referent, cohort, rhyme or unrelated. The type of the object is
        used to determine the condition of the trial.
      \item
        \textbf{Condition} : The condition of the trial.
      \item
        \textbf{Target} : The target object that the participants have
        to click on. You can find the stimuli.csv file
        \href{stimuli/stimuli-final.csv}{here}.
      \end{itemize}
    \item
      We also have accompaning audio stimuli for each trial. The audio
      is digitally recorded. The audio stimuli are recorded in the
      following format:

      \begin{itemize}
      \tightlist
      \item
        \textbf{Instruction} : `Fixate at the center' , `Now click on
        the \{target\}'
      \item
        \textbf{Target} : The target object that the participants have
        to click on. You can find the audio stimuli
        \href{stimuli/audio-stimuli}{here}. Each response is captured
        with a mouse click. The mouse click is recorded and logged using
        the mouse\_response item in OpenSesame.
      \end{itemize}
    \end{enumerate}
  \item
    \textbf{Logging} : The onset and offset of the fixation cross and
    the stimulus (both audio and visual) is logged for each trail. We
    also log the position of the mouse click and the target object along
    with its position (top,right, buttom, left) that the participants
    clicked on. (\emph{More details will be proviided in the
    preprocessing and analysis section}.)
  \item
    \textbf{Gaze Contingency} : Two fixation audio prompts which says
    \emph{`Fixate at the center'} and `\emph{Now fixate at the center}'
    and marks the beginning and end of one trail.The fixation audio
    prompt is played to ensure that the participants are looking at the
    center of the screen before the spoken instruction is played. This
    is implemented by introducing a delay of 1.1s after the prompt.
    \includegraphics[width=0.9\textwidth,height=\textheight]{img/trial_timeline.png}
  \end{itemize}
\item
  \textbf{End of Experiment} : The experiment ends with a thank you
  message for the participants.
\end{enumerate}

\hypertarget{logic-of-the-experiment}{%
\subsection{Logic of the experiment}\label{logic-of-the-experiment}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Stimuli are chosen as per the different pairs of sets included in the
  reference paper by Paul D.Allopenna, James S. Magnuson and Michael K.
  Tanenhaus.

  \begin{itemize}
  \tightlist
  \item
    For eg. One criteria of choosing those words are based on frequency
    per million words in the Kucera and Francis, 1967, corpus.
  \end{itemize}
\item
  A fixation at any point on the screen indicates that the participant
  is paying attention to it. Thus we record the fixations throughout the
  experiment to deduce the attention of the pariticipant when we
  instruct them to fixate or look at a certain point of the canvas.
\item
  Noting timestamps of the samples is essential. Our experiment is
  designed to record how a participants attends to the stimuli and how
  they respond to the spoken instruction while the instruction is
  unfolding. Thus we record the timestamps of the samples to understand
  the chronology of the fixation events.
\item
  Fixations at the centre of the screen marks the start and end of a
  trial. This is to make sure we don't overlap the data of two trials
  while recording the data since each participant will have different
  response times and thus a fixed duration for each trial for timeout
  will not be feasible.
\end{enumerate}

\hypertarget{stimulus-design}{%
\section{Stimulus Design}\label{stimulus-design}}

\hypertarget{stimuli-preprocessing}{%
\section{Stimuli Preprocessing}\label{stimuli-preprocessing}}

The stimuli collected needed to be preprocessed before they could be
used in the experiment. The preprocessing steps are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Resizing the line drawings into 256x256 pixels. The OpenCV library was
  was used to read and resize the images.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{img }\OperatorTok{=}\NormalTok{ cv2.resize(img, size)}
\end{Highlighting}
\end{Shaded}
\item
  Converting the audio files to \texttt{.wav} format. The audio files
  were generated in \texttt{.mp3} format. The

\begin{Shaded}
\begin{Highlighting}[]
    \ControlFlowTok{for}\NormalTok{ file }\KeywordTok{in} \VariableTok{$DIRPATH}\NormalTok{/}\PreprocessorTok{*}\NormalTok{.mp3}\KeywordTok{;} \ControlFlowTok{do}
        \VariableTok{filename}\OperatorTok{=}\VariableTok{$(}\FunctionTok{basename} \StringTok{"}\VariableTok{$file}\StringTok{"}\VariableTok{)}
        \VariableTok{filename}\OperatorTok{=}\StringTok{"}\VariableTok{$\{filename}\OperatorTok{\%}\NormalTok{.}\PreprocessorTok{*}\VariableTok{\}}\StringTok{"}
        \ExtensionTok{ffmpeg} \AttributeTok{{-}i} \VariableTok{$file} \VariableTok{$OUTDIR}\NormalTok{/}\VariableTok{$filename}\NormalTok{.wav}
    \ControlFlowTok{done}
\end{Highlighting}
\end{Shaded}
\item
  A trailing silence after each audio was observed which would affect
  the response time of the participants. The silence was removed using
  the \texttt{pydub} library.

\begin{Shaded}
\begin{Highlighting}[]
    \KeywordTok{def}\NormalTok{ detect\_leading\_silence(sound, silence\_threshold, chunk\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{):}
\NormalTok{        trim\_ms }\OperatorTok{=} \DecValTok{0} \CommentTok{\# ms}
        \ControlFlowTok{while}\NormalTok{ sound[trim\_ms:trim\_ms}\OperatorTok{+}\NormalTok{chunk\_size].dBFS }\OperatorTok{\textless{}}\NormalTok{ silence\_threshold:}
\NormalTok{            trim\_ms }\OperatorTok{+=}\NormalTok{ chunk\_size}
        \ControlFlowTok{return}\NormalTok{ trim\_ms}
\end{Highlighting}
\end{Shaded}

  The function analyzes an audio snippet to find the duration of the
  silence at the beginning of the signal. It iterates over chunks of the
  audio and measuring the volume (dBFS) in each chunk until the volume
  exceeds the provided silence threshold. The accumulated time of
  trimmed silence is then returned as the result and then removed using
  the \texttt{sound{[}trim\_ms:{]}} function, spectifying the start and
  the end trim duration.
\item
  The sampling rate of all the audio samples was also made equal to work
  with the \texttt{PsychoPy} backend. The sampling rate was changed to
  \textless\textgreater{} using the \textless\textgreater.
\end{enumerate}

\hypertarget{conditions}{%
\section{Conditions}\label{conditions}}

\hypertarget{randomization-and-quality-control}{%
\section{Randomization and Quality
Control}\label{randomization-and-quality-control}}

\hypertarget{experiment-organization}{%
\section{Experiment Organization}\label{experiment-organization}}

\hypertarget{preprocessing-and-analysis}{%
\section{Preprocessing and Analysis}\label{preprocessing-and-analysis}}

\hypertarget{preprocessing-and-analysis-1}{%
\section{Preprocessing and
Analysis}\label{preprocessing-and-analysis-1}}

\hypertarget{organizing-the-raw-data-into-trial-wise-data}{%
\subsection{Organizing the raw data into trial-wise
data}\label{organizing-the-raw-data-into-trial-wise-data}}

The basic unit of analysis in the visual world paradigm experiment is a
trial. A trial is a single instance of the experiment. The first task
was to organize the raw data into trial-wise data. Here the raw data was
present in the .tsv files. Each file contained the data for a single
participant. The data was read into a pandas dataframe named
\texttt{df\_interest} and then the columns that were not required were
dropped. The columns, \texttt{TIME}, \texttt{BPOGX}, \texttt{BPOGY},
\texttt{FPOGD}, \texttt{FPOGX}, \texttt{FPOGY}, \texttt{FPOGV} and
\texttt{USER} were relevant for our analysis so these were the columns
remaining in the dataframe.

In order to organize the entries of the dataframe into trials, the rows
corresponding to the start and end of the trials needed to be
identified. After acquiring the indices of the corresponding rows, the
dataframe was split into multiple dataframes, each corresponding to a
single trial.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get the indices of the rows where the user column contains the phrase \textquotesingle{}START\_TRIAL\textquotesingle{}}
\NormalTok{start\_indices }\OperatorTok{=}\NormalTok{ df\_interest[df\_interest[}\StringTok{\textquotesingle{}USER\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.contains(}\StringTok{\textquotesingle{}START\_TRIAL\textquotesingle{}}\NormalTok{)].index}

\CommentTok{\# get the indices of the rows where the user column contains the phrase \textquotesingle{}FINAL\_FIXATION\_END\textquotesingle{}}
\NormalTok{end\_indices }\OperatorTok{=}\NormalTok{ df\_interest[df\_interest[}\StringTok{\textquotesingle{}USER\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.contains(}\StringTok{\textquotesingle{}FINAL\_FIXATION\_END\textquotesingle{}}\NormalTok{)].index}

\CommentTok{\# split the dataframe based on the start and end indices}
\NormalTok{df\_list }\OperatorTok{=}\NormalTok{ [df\_interest.iloc[start\_indices[i]:end\_indices[i]] }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(start\_indices))]}
\end{Highlighting}
\end{Shaded}

At this point, the dataframes corresponding to the individual trials
were ready. Our analysis is mainly concerned with the gaze data from the
point of onset of the audio stimilus up till the point at which the
participant clicks on a stimulus. So, the dataframes were further sliced
to retain only the data from the specified interval. In order to perform
this, the \texttt{USER} column was used. The rows corresponding to the
start of the audio stimulus and the end of the click response were
identified by the strings \texttt{LOG\_AUDIO\_TARGET\_START} and
\texttt{CLICK\_RESPONSE\_END} respectively. The rows between these two
rows were sliced and the resulting dataframes were stored in a list
named \texttt{audio\_df\_list}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# extract the row index where the user column contains the phrase \textquotesingle{}LOG\_AUDIO\_TARGET\_START\textquotesingle{}}
\NormalTok{audio\_start\_index }\OperatorTok{=}\NormalTok{ selected\_df[selected\_df[}\StringTok{\textquotesingle{}USER\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.contains(}\StringTok{\textquotesingle{}LOG\_AUDIO\_TARGET\_START\textquotesingle{}}\NormalTok{)].index[}\DecValTok{0}\NormalTok{]}
\CommentTok{\# extract the row index where the user column contains the phrase \textquotesingle{}LOG\_AUDIO\_TARGET\_END\textquotesingle{}}
\NormalTok{audio\_end\_index }\OperatorTok{=}\NormalTok{ selected\_df[selected\_df[}\StringTok{\textquotesingle{}USER\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.contains(}\StringTok{\textquotesingle{}CLICK\_RESPONSE\_END\textquotesingle{}}\NormalTok{)].index[}\DecValTok{0}\NormalTok{]}
\CommentTok{\# split the dataframe based on the audio start and end indices}
\CommentTok{\# store the split dataframe in a list}
\NormalTok{audio\_df\_list.append(selected\_df.iloc[audio\_start\_index:audio\_end\_index }\OperatorTok{+} \DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\hypertarget{extracting-trial-information}{%
\subsection{Extracting trial
information}\label{extracting-trial-information}}

The logs present in the .tsv files are important for our analysis. Apart
from containing the data recordings from the experiments, they also
contain the information about the individual trials. In the text block
below, the logs for a sample trial are shown.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{START\_EXP}
\NormalTok{START\_TRIAL: 0 T: SADDLE.PNG R: PICKLE.PNG B: PADLOCK.PNG L: CANDY.PNG}
\NormalTok{FIXATE\_CENTER\_AUDIO\_ONSET, COND: 12 TARGET: CANDY}
\NormalTok{CENTRE\_GAZE\_START}
\NormalTok{INSTRUCTION\_TO\_CLICK\_ONSET}
\NormalTok{LOG\_AUDIO\_TARGET\_START}
\NormalTok{LOG\_AUDIO\_TARGET\_END}
\NormalTok{CLICK\_RESPONSE\_END}
\NormalTok{FINAL\_FIXATION\_START, SELECTED: CANDY.PNG}
\NormalTok{FINAL\_FIXATION\_END}
\NormalTok{….}
\NormalTok{….}
\NormalTok{….}
\NormalTok{….}
\NormalTok{STOP\_EXP}
\end{Highlighting}
\end{Shaded}

The logs are in the form of a sequence of events. Each log event is a
line in the log file. Out of these log events, the following ones are
relevant for our analysis:

\begin{itemize}
\tightlist
\item
  \texttt{START\_TRIAL:\ 0\ T:\ SADDLE.PNG\ R:\ PICKLE.PNG\ B:\ PADLOCK.PNG\ L:\ CANDY.PNG}
  : This line contains the information about the trial. The trial number
  is 0. The images on the left, right, top and bottom of the target
  image are \texttt{SADDLE.PNG}, \texttt{PICKLE.PNG},
  \texttt{PADLOCK.PNG} and \texttt{CANDY.PNG} respectively.
\item
  \texttt{FIXATE\_CENTER\_AUDIO\_ONSET,\ COND:\ 12\ TARGET:\ CANDY}:
  Other than indicating the onset of the audio for fixation, this line
  also contains the target word and the condition number. In this case,
  the target word is \texttt{CANDY} and the condition number is 12.
\item
  \texttt{FINAL\_FIXATION\_START,\ SELECTED:\ CANDY.PNG}: This line
  indicates the onset of the final fixation on the target image and the
  stimulus that was selected. The participant selected the image
  \texttt{CANDY.PNG} as the target image.
\end{itemize}

Following the slicing procedure mentioned the previous section, the
indices of the rows corresponding to these three log events are
retrieved.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get all rows whose indices are stored in start\_indices}
\CommentTok{\# will be used to extract the position of the stimuli}
\NormalTok{trial\_strings }\OperatorTok{=}\NormalTok{ df\_interest.iloc[start\_indices][}\StringTok{\textquotesingle{}USER\textquotesingle{}}\NormalTok{].reset\_index(drop}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# get the indices of rows that contain the phrase \textquotesingle{}FIXATE\_CENTER\_AUDIO\_ONSET\textquotesingle{}}
\NormalTok{target\_row\_indices }\OperatorTok{=}\NormalTok{ df\_interest[df\_interest[}\StringTok{\textquotesingle{}USER\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.contains(}\StringTok{\textquotesingle{}FIXATE\_CENTER\_AUDIO\_ONSET\textquotesingle{}}\NormalTok{)].index}
\NormalTok{target\_rows }\OperatorTok{=}\NormalTok{ df\_interest.iloc[target\_row\_indices].reset\_index(drop}\OperatorTok{=}\VariableTok{True}\NormalTok{)[}\StringTok{\textquotesingle{}USER\textquotesingle{}}\NormalTok{]}

\CommentTok{\# get the indices of rows that contain the phrase \textquotesingle{}FINAL\_FIXATION\_START\textquotesingle{}}
\NormalTok{fixation\_row\_indices }\OperatorTok{=}\NormalTok{ df\_interest[df\_interest[}\StringTok{\textquotesingle{}USER\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.contains(}\StringTok{\textquotesingle{}FINAL\_FIXATION\_START\textquotesingle{}}\NormalTok{)].index}
\NormalTok{fixation\_rows }\OperatorTok{=}\NormalTok{ df\_interest.iloc[fixation\_row\_indices].reset\_index(drop}\OperatorTok{=}\VariableTok{True}\NormalTok{)[}\StringTok{\textquotesingle{}USER\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

The retrieved data were all of the datatype string so regex was used to
extract the data points of interest. This consisted of the name of the
stimulus at the top, bottom, left and right positions of the grid, the
target word, the condition number and the selected stimulus. The
extracted data were stored in a python dictionary named
\texttt{stimuli\_loc\_dict} with appropriate keys.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# use regex to extract the number afer \textquotesingle{}COND:\textquotesingle{}}
\NormalTok{cond\_numbers }\OperatorTok{=}\NormalTok{ [re.findall(}\VerbatimStringTok{r\textquotesingle{}COND: (\textbackslash{}d+)\textquotesingle{}}\NormalTok{, row)[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in}\NormalTok{ target\_rows]}
\CommentTok{\# use regex to extract the word after \textquotesingle{}TARGET:\textquotesingle{}}
\NormalTok{target\_words }\OperatorTok{=}\NormalTok{ [re.findall(}\VerbatimStringTok{r\textquotesingle{}TARGET: (\textbackslash{}w+)\textquotesingle{}}\NormalTok{, row)[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in}\NormalTok{ target\_rows]}
\CommentTok{\# use regex to extract the word after \textquotesingle{}SELECTED: \textquotesingle{}}
\NormalTok{selected\_words }\OperatorTok{=}\NormalTok{ [re.findall(}\VerbatimStringTok{r\textquotesingle{}SELECTED: (\textbackslash{}w+)\textquotesingle{}}\NormalTok{, row)[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in}\NormalTok{ fixation\_rows]}

\CommentTok{\# use regex to extract the image names at the top, bottom, right and left positions}
\NormalTok{top\_stimuli }\OperatorTok{=}\NormalTok{ [re.findall(}\VerbatimStringTok{r\textquotesingle{}T: (\textbackslash{}w+)\textquotesingle{}}\NormalTok{, trial\_string)[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ trial\_string }\KeywordTok{in}\NormalTok{ trial\_strings]}
\NormalTok{bottom\_stimuli }\OperatorTok{=}\NormalTok{ [re.findall(}\VerbatimStringTok{r\textquotesingle{}B: (\textbackslash{}w+)\textquotesingle{}}\NormalTok{, trial\_string)[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ trial\_string }\KeywordTok{in}\NormalTok{ trial\_strings]}
\NormalTok{right\_stimuli }\OperatorTok{=}\NormalTok{ [re.findall(}\VerbatimStringTok{r\textquotesingle{}R: (\textbackslash{}w+)\textquotesingle{}}\NormalTok{, trial\_string)[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ trial\_string }\KeywordTok{in}\NormalTok{ trial\_strings]}
\NormalTok{left\_stimuli }\OperatorTok{=}\NormalTok{ [re.findall(}\VerbatimStringTok{r\textquotesingle{}\textbackslash{}sL: (\textbackslash{}w+)\textquotesingle{}}\NormalTok{, trial\_string)[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ trial\_string }\KeywordTok{in}\NormalTok{ trial\_strings]}
\end{Highlighting}
\end{Shaded}

The gaze data contained in \texttt{audio\_df\_list} provided the
coordinates where the participant was fixating at a given timestamp but
for our task we needed to know which stimulus the participant was
fixating at. The coordinates of the grid boxes were noted from the
OpenSesame experiment UI. But one issues with this data is that these
coordinates had the origin at the center of the screen whereas the gaze
data had the origin at the top left corner of the screen. So, the
coordinates of the grid boxes were converted to the coordinate system of
the gaze data and then scaled to the range {[}0, 1{]}\footnote{The
  scaling is performed with regard to the resolution of a screen
  resolution of 1920x1080. Hence, a maximum value of 1 along height and
  width correspond to 1080 and 1920 respectively.}. The functions
\texttt{shift\_coordinate\_system} and
\texttt{shift\_coordinate\_system\_single} were defined for this
purpose. The function \texttt{shift\_coordinate\_system} accepted a
dictionary of coordinates while the function
\texttt{shift\_coordinate\_system\_single} accepted a single set of
coordinates (tuple). The functions returned the shifted coordinates.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# shift the origin from (0, 0) to ({-}960, 540)}
\CommentTok{\# perform the same on outer\_points and inner\_points}
\KeywordTok{def}\NormalTok{ shift\_coordinate\_system(coord\_dict):}
    \ControlFlowTok{for}\NormalTok{ key, value }\KeywordTok{in}\NormalTok{ coord\_dict.items():}
\NormalTok{        coord\_dict[key] }\OperatorTok{=}\NormalTok{ (value[}\DecValTok{0}\NormalTok{] }\OperatorTok{+} \DecValTok{960}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1} \OperatorTok{*}\NormalTok{ value[}\DecValTok{1}\NormalTok{] }\OperatorTok{+} \DecValTok{540}\NormalTok{)}
    
        \CommentTok{\# scale to [0, 1]}
\NormalTok{        coord\_dict[key] }\OperatorTok{=}\NormalTok{ (coord\_dict[key][}\DecValTok{0}\NormalTok{] }\OperatorTok{/} \DecValTok{1920}\NormalTok{, coord\_dict[key][}\DecValTok{1}\NormalTok{] }\OperatorTok{/} \DecValTok{1080}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ coord\_dict}

\KeywordTok{def}\NormalTok{ shift\_coordinate\_system\_single(coord):}
\NormalTok{    coord }\OperatorTok{=}\NormalTok{ (coord[}\DecValTok{0}\NormalTok{] }\OperatorTok{+} \DecValTok{960}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1} \OperatorTok{*}\NormalTok{ coord[}\DecValTok{1}\NormalTok{] }\OperatorTok{+} \DecValTok{540}\NormalTok{)}
\NormalTok{    coord }\OperatorTok{=}\NormalTok{ (coord[}\DecValTok{0}\NormalTok{] }\OperatorTok{/} \DecValTok{1920}\NormalTok{, coord[}\DecValTok{1}\NormalTok{] }\OperatorTok{/} \DecValTok{1080}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ coord}
\end{Highlighting}
\end{Shaded}

The \texttt{shift\_coordinate\_system} function can convert OpenSesame
UI coordinates to the cartesian coordinate system (origin on the bottom
left corner) and scale them to the range \texttt{{[}0,\ 1{]}}. The gaze
data acquired from the GP3 eye tracker follows a different coordinate
system. The origin of the gaze data coordinate system is at the top left
corner of the screen. Additionally, the y-axis is inverted, meaning that
the y-coordinate increases as the participant looks down. In order to
convert the gaze data to the cartesian coordinate system in order to
enable comparison with the transformed OpenSesame UI coordinates, the
function \texttt{shift\_coordinate\_system\_bottom\_left\_to\_top\_left}
was defined. The scaled version of the cartesian coordinates was chosen
in order to enable use with plotting libraries such as \emph{matplotlib}
and \emph{seaborn}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ shift\_coordinate\_system\_bottom\_left\_to\_top\_left(x, y):}
    \ControlFlowTok{return}\NormalTok{ (x, }\OperatorTok{{-}}\DecValTok{1} \OperatorTok{*}\NormalTok{ y }\OperatorTok{+} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, opacitybacktitle=0.6, left=2mm, toptitle=1mm, leftrule=.75mm, opacityback=0, breakable, bottomrule=.15mm, colframe=quarto-callout-note-color-frame, rightrule=.15mm, coltitle=black, bottomtitle=1mm, toprule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Nota Bene}, arc=.35mm]

The GP3 gaze data coordinates are in the range \texttt{{[}0,\ 1{]}} so
no scaling is required.

\end{tcolorbox}

\hypertarget{fixation-plots}{%
\subsection{Fixation plots}\label{fixation-plots}}

The preprocessing steps described in the previous sections are performed
on the recorded data, it is possible to plot the fixations of the
participants . Such plots allow us visualize the fixations of the
participants and identify any outliers. The plot elements can be
classified into two groups:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Overlay elements: These elements are plotted in order to provide
  reference for the position of the grid and indicate the stimulus image
  in each grid box element. The condition number and the target word are
  also displayed in the plot. The function \texttt{draw\_grid} is used
  to draw the grid.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ draw\_grid(inn, out, ax):}
    \CommentTok{\# draw line from A to B}
\NormalTok{    ax.plot([out[}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{], out[}\StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{]], [out[}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{][}\DecValTok{1}\NormalTok{], out[}\StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{][}\DecValTok{1}\NormalTok{]], color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{)}
    \CommentTok{\# draw line from B to C}
\NormalTok{    ax.plot([out[}\StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{], out[}\StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{]], [out[}\StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{][}\DecValTok{1}\NormalTok{], out[}\StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{][}\DecValTok{1}\NormalTok{]], color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{)}
    \CommentTok{\# draw line from C to D}
\NormalTok{    ...}
\NormalTok{    ...}
    \CommentTok{\# create a tiny circle at the center}
\NormalTok{    ax.scatter(inn[}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{], inn[}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{][}\DecValTok{1}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The text elements are plotted using \texttt{matplotlib.pyplot.text()}
function. See example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# top stimuli}
\NormalTok{ax.text(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.8685}\NormalTok{, stimuli\_dict[i][}\DecValTok{0}\NormalTok{].lower(), transform}\OperatorTok{=}\NormalTok{ax.transAxes, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    verticalalignment}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, bbox}\OperatorTok{=}\NormalTok{props, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Fixation elements: These elements are plotted in order to indicate the
  fixations of the participants. As indicated in the code block below,
  the matplotlib scatter function is used.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# new\_fpog\_x and new\_fpog\_y are the x and y coordinates of the fixations}
\NormalTok{ax.scatter(new\_fpog\_x, new\_fpog\_y, color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, opacitybacktitle=0.6, left=2mm, toptitle=1mm, leftrule=.75mm, opacityback=0, breakable, bottomrule=.15mm, colframe=quarto-callout-note-color-frame, rightrule=.15mm, coltitle=black, bottomtitle=1mm, toprule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Nota Bene}, arc=.35mm]

The fixation plots can be generated by running the script
\texttt{generate\_fixation\_plots.py} in the \texttt{src} directory.

\end{tcolorbox}

\hypertarget{deduce-location-of-fixations}{%
\subsection{Deduce location of
fixations}\label{deduce-location-of-fixations}}

Using the coordinates of the edges of the grid boxes, it is possible to
deduce the location of the fixations. The gridbox has four boxes that
where a stimulus can be placed. The coordinates of the fixations and the
coordinates of the stimulus boxes are converted to the scaled cartesian
coordinate system. The function \texttt{check\_if\_within\_rect} accepts
the x and y coordinates of the fixation and the coordinates of the
stimulus box and returns a boolean value indicating whether the fixation
is within the stimulus box. The function
\texttt{check\_if\_within\_rect} is called for each stimulus box and the
stimulus box for which the function returns \texttt{True} is the
stimulus box where the participant was fixating.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ get\_rect(x, y):}
    \ControlFlowTok{if}\NormalTok{ check\_if\_within\_rect(x, y, top\_rect):}
        \ControlFlowTok{return} \StringTok{\textquotesingle{}top\textquotesingle{}}
    \ControlFlowTok{elif}\NormalTok{ check\_if\_within\_rect(x, y, right\_rect):}
        \ControlFlowTok{return} \StringTok{\textquotesingle{}right\textquotesingle{}}
    \ControlFlowTok{elif}\NormalTok{ check\_if\_within\_rect(x, y, bottom\_rect):}
        \ControlFlowTok{return} \StringTok{\textquotesingle{}bottom\textquotesingle{}}
    \ControlFlowTok{elif}\NormalTok{ check\_if\_within\_rect(x, y, left\_rect):}
        \ControlFlowTok{return} \StringTok{\textquotesingle{}left\textquotesingle{}}
    \ControlFlowTok{elif}\NormalTok{ check\_if\_within\_rect(x, y, centre\_rect):}
        \ControlFlowTok{return} \StringTok{\textquotesingle{}centre\textquotesingle{}}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{\textquotesingle{}outside\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

The function \texttt{get\_rect} is applied to each row of the dataframe
using the \texttt{df.apply} function. The resulting column is named
\texttt{rect}. At the point, for each data point, we know at which
stimulus box the participant was fixating.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# use df.apply to apply the get\_rect function to each row}
\NormalTok{audio\_df\_valid\_fixation[}\StringTok{\textquotesingle{}rect\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ audio\_df\_valid\_fixation.}\BuiltInTok{apply}\NormalTok{(}\KeywordTok{lambda}\NormalTok{ row: get\_rect(row[}\StringTok{\textquotesingle{}FPOGX\textquotesingle{}}\NormalTok{], row[}\StringTok{\textquotesingle{}FPOGY\textquotesingle{}}\NormalTok{]), axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{mapping-stimulus-location-to-stimulus-type}{%
\subsection{Mapping stimulus location to stimulus
type}\label{mapping-stimulus-location-to-stimulus-type}}

The analysis plot is concerned with the stimulus type rather than the
stimulus location. As the fixations have been mapped to the stimulus
location, using the data available in the csv logfile, it is possible to
map the stimulus location to the stimulus type for each trial. The csv
logfile contains the following columns, \emph{referant, cohort, rhyme,
distractor, target, trial number} and \emph{condition number}. Each row
indicates the names of the stimulus that was assigned the role of
referant, cohort, rhyme, etc., for a given trial.

\hypertarget{challenges-and-limitations}{%
\section{Challenges and Limitations}\label{challenges-and-limitations}}

\hypertarget{challenges}{%
\subsection{Challenges}\label{challenges}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Balancing of trials}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Challenge} :Our stimuli had a total of \emph{23 unique
    visual stimuli}, \emph{12 different conditions} and \emph{4
    different types}. Adhering to proper randomization and balancing of
    trials was a challenge as we had to ensure that each participant saw
    the same number of trials for each condition and each type and such
    that each object appeared equal number of times.
  \item
    \textbf{Solution} : Although we started tith a python script to
    generate the trials, we had to revert to manual creation of trials
    as the script was not able to generate balanced trials which was a
    primary requirement for our experiment.
  \end{itemize}
\item
  \textbf{Random `freezes' during experiment} :

  \begin{itemize}
  \item
    \textbf{Challenge} : The experiment encountered random freezes
    during its run. The freezes were random and could not be reproduced.
    This was a major challenge as we had to restart the experiment from
    the beginning. The most frequent freezes was found on the \emph{gaze
    contingency} check, which allowed the experiment to move forward
    only when the participant was looking at the center of the screen.
  \item
    \textbf{Trails} :

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      While True loop
    \end{enumerate}

\begin{verbatim}
    while True:
    gazepos = eyetracker.sample()
\end{verbatim}

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \setcounter{enumii}{1}
    \tightlist
    \item
      Periodic sampling
    \end{enumerate}

\begin{verbatim}
    while True:
        if clock.time() - check_timer > diff:
            gazepos = eyetracker.sample()
\end{verbatim}

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \setcounter{enumii}{2}
    \item
      Switching backend to PsychoPy Made the sample rates of all the
      audio samples equal to work with the PsychoPy backend.
    \item
      Number of trials Reduced the number of trials to 24 from 12 to
      reduce the time of the experiment but the problem still persisted.
    \end{enumerate}
  \item
    \textbf{Solution} : Removal of gaze contingent features and instead
    introducing a delay to ensure that the participants are looking at
    the center of the screen before the spoken instruction is played.
    This was implemented by introducing a delay of 1.1s after the
    prompt.
  \end{itemize}
\item
  \textbf{Eye tracker calibration} :

  \begin{itemize}
  \tightlist
  \item
    \textbf{Challenge} : The eye tracker calibration was a challenge as
    the participants were not able to calibrate the eye tracker
    properly, due to many reasons like contact lenses, glasses, height
    of the participants, body posture during the experiment etc.
  \item
    \textbf{Solution} : THe number of participants were increased to 16
    to ensure that we have atleast 12 participants with proper
    calibration and the timing of one full experiment was reduced to a
    maximum of 8 minutes to avoid recalibration in between the
    experiments. The participants were also given a practice trial to
    ensure that they are comfortable with the experiment and the eye
    tracker calibration.
  \end{itemize}
\end{enumerate}

\hypertarget{limitations}{%
\subsection{Limitations}\label{limitations}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Since Visual World Paradigm is a well known experimental framework, it
  is possible that the participants might have been aware of the purpose
  of the experiment and thus might have been biased in their responses
  despite the large number of filler trials. Also, we used a relatively
  small set of pictures, which might have led to a learning effect i.e
  the participants might have been able to predict the target word based
  on the previous trials.
\item
  Generalizability is affected as our study participants only include
  university students of a specific age group, which does not fully
  represent the complexities and variations of real-world spoken word
  recognition scenarios. The results may not be applicable to other age
  groups or people with different educational backgrounds who might have
  more or less exposure to the field of cognitive psychology.
\item
  The study also may not fully address the universality of the observed
  effects across different languages as the original study as well as
  our replication is in English.
\end{enumerate}

References: (Vitay, 2017) linguistics and See Figure~\ref{fig-matrix}
and Section~\ref{sec-results}.

\[
    \tau \, \frac{dx_j(t)}{dt} + x_j(t)= \sum_i w^{in}_{ij} \, r^{in}_i(t) + g \, \sum_{i \neq j} w^{rec}_{ij} \, r_i(t)
\]

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, titlerule=0mm, opacitybacktitle=0.6, left=2mm, toptitle=1mm, leftrule=.75mm, opacityback=0, breakable, bottomrule=.15mm, colframe=quarto-callout-note-color-frame, rightrule=.15mm, coltitle=black, bottomtitle=1mm, toprule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Nota Bene}, arc=.35mm]

Important information.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{10}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(i)}
\end{Highlighting}
\end{Shaded}

\hypertarget{second-subsection}{%
\subsection{Second subsection}\label{second-subsection}}

\url{https://www.youtube.com/embed/tPgf_btTFlc}

\hypertarget{material-and-methods}{%
\section{Material and methods}\label{material-and-methods}}

\hypertarget{sec-results}{%
\section{Results}\label{sec-results}}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-Vitay2017}{}}%
Vitay, J. (2017). On the role of dopamine in motivated behavior: A
neuro-computational approach. Available at:
\url{https://julien-vitay.net/publication/vitay2017/}.

\end{CSLReferences}



\end{document}
