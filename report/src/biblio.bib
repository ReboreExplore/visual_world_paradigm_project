
@article{allopenna_tracking_1998,
	title = {Tracking the {Time} {Course} of {Spoken} {Word} {Recognition} {Using} {Eye} {Movements}: {Evidence} for {Continuous} {Mapping} {Models}},
	volume = {38},
	issn = {0749-596X},
	shorttitle = {Tracking the {Time} {Course} of {Spoken} {Word} {Recognition} {Using} {Eye} {Movements}},
	url = {https://www.sciencedirect.com/science/article/pii/S0749596X97925584},
	doi = {10.1006/jmla.1997.2558},
	abstract = {Eye movements to pictures of four objects on a screen were monitored as participants followed a spoken instruction to move one of the objects, e.g., “Pick up the beaker; now put it below the diamond” (Experiment 1) or heard progressively larger gates and tried to identify the referent (Experiment 2). The distractor objects included a cohort competitor with a name that began with the same onset and vowel as the name of the target object (e.g.,beetle), a rhyme competitor (e.g.speaker), and an unrelated competitor (e.g.,carriage). In Experiment 1, there was clear evidence for both cohort and rhyme activation as predicted by continuous mapping models such as TRACE (McClelland and Elman, 1986) and Shortlist (Norris, 1994). Additionally, the time course and probabilities of eye movements closely corresponded to response probabilities derived from TRACE simulations using the Luce choice rule (Luce, 1959). In the gating task, which emphasizes word-initial information, there was clear evidence for multiple activation of cohort members, as measured by judgments and eye movements, but no suggestion of rhyme effects. Given that the same sets of pictures were present during the gating task as in Experiment 1, we conclude that the rhyme effects in Experiment 1 were not an artifact of using a small set of visible alternatives.},
	number = {4},
	urldate = {2023-08-30},
	journal = {Journal of Memory and Language},
	author = {Allopenna, Paul D. and Magnuson, James S. and Tanenhaus, Michael K.},
	month = may,
	year = {1998},
	pages = {419--439},
}

@article{mathot_opensesame:_2012,
	title = {{OpenSesame}: {An} open-source, graphical experiment builder for the social sciences},
	volume = {44},
	issn = {1554-3528},
	shorttitle = {{OpenSesame}},
	url = {https://doi.org/10.3758/s13428-011-0168-7},
	doi = {10.3758/s13428-011-0168-7},
	abstract = {In the present article, we introduce OpenSesame, a graphical experiment builder for the social sciences. OpenSesame is free, open-source, and cross-platform. It features a comprehensive and intuitive graphical user interface and supports Python scripting for complex tasks. Additional functionality, such as support for eyetrackers, input devices, and video playback, is available through plug-ins. OpenSesame can be used in combination with existing software for creating experiments.},
	language = {en},
	number = {2},
	urldate = {2023-08-30},
	journal = {Behavior Research Methods},
	author = {Mathôt, Sebastiaan and Schreij, Daniel and Theeuwes, Jan},
	month = jun,
	year = {2012},
	keywords = {Software, Stimulus presentation, Experiment builder, Python, Graphical user interface},
	pages = {314--324},
}

@online{visual-world-paradigm,
    author = "Stephen Politzer-Ahles",
    title = "Visual World Paradigm",
    howpublished = "Webpage",
    url = "https://people.ku.edu/~sjpa/Classes/CBS592/EyeTracking/visual-world-paradigm.html",
}

@manual{gazepoint_api,
  title = {Gazepoint API v2.0 Documentation},
  author = {Gazepoint},
  howpublished = {\url{https://www.gazept.com/dl/Gazepoint_API_v2.0.pdf}},
  year = {2013},
}

